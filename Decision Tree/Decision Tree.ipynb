{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID3/ C4.5/ CART \n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Decision Tree Classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ------------------\n",
    "    verbose: int {1,0}\n",
    "        0. Print nothing\n",
    "        1. Print each steps\n",
    "        \n",
    "    criterionï¼šstring\n",
    "        1. Gini: gini impurity\n",
    "        2. InfoGain: information gain\n",
    "        3. GainRatio: information gain ratio\n",
    "    \n",
    "    max_depth: int\n",
    "        The maximum depth of the tree. \n",
    "        If None, then nodes are expanded until all leaves are pure \n",
    "        or until all leaves contain less than min_samples_split samples.\n",
    "    \n",
    "    min_impurity_decrease:\n",
    "    \n",
    "    min_samples_leaf\n",
    "    \n",
    "    max_leaf_nodes\n",
    "    \n",
    "    min_impurity_split\n",
    "    \n",
    "    \n",
    "        \n",
    "    Attributes\n",
    "    ------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self,criterion = 'InfoGain', verbose = 1):\n",
    "        self.criterion = criterion\n",
    "        if self.criterion == 'InfoGain':\n",
    "            self.feature_split = self._feature_Select_Info_Gain\n",
    "        elif self.criterion == 'GainRatio':\n",
    "            self.feature_split = self._feature_Select_Info_Gain_Ratio\n",
    "        elif self.criterion == 'Gini':\n",
    "            self.feature_split = self._feature_Select_Gini\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    \"\"\"\n",
    "    Step1: FEATURE SELECTION\n",
    "    \n",
    "    Calculate Entropy or Gini index \n",
    "\n",
    "    \"\"\"\n",
    "    # Calculate entropy\n",
    "    \n",
    "    def calcShannonEntropy(self,y_labels):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------------\n",
    "        y_labels: {array-like}, shape = [n_samples, 1]\n",
    "            Dateset matrix, where 'n_samples' is the number of samples \n",
    "            and one column of labels\n",
    "        \n",
    "        Return\n",
    "        ------------------\n",
    "        shannonEntropy: float\n",
    "        \n",
    "        \"\"\"\n",
    "        m = len(y_labels)\n",
    "        \n",
    "        labelCounts = collections.defaultdict(int)\n",
    "        \n",
    "        # the number of unique elements and their occurrence\n",
    "        for label in y_labels:\n",
    "\n",
    "            if label not in labelCounts:\n",
    "                labelCounts[label] = 0\n",
    "            labelCounts[label] += 1\n",
    "            \n",
    "        # calculate shannon entropy\n",
    "        shannonEntropy = 0.0\n",
    "        for key in labelCounts:\n",
    "            # calculate occurrence\n",
    "            prob = float(labelCounts[key]/m)\n",
    "            if prob == 0:\n",
    "                continue\n",
    "            # calculate entropy\n",
    "            shannonEntropy -= prob * np.log2(prob)\n",
    "        \n",
    "        return shannonEntropy\n",
    "    \n",
    "    # calculate conditional entropy\n",
    "    def conditionalEnropy(self, X, y_label, feature, split_point):#, split_point\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------------\n",
    "        X: {array-like}\n",
    "        preprocessed dataset, could not deal with continuous features, \n",
    "        and categorical feature should be better as binary form\n",
    "        \n",
    "        y_label: {array-like}\n",
    "        \n",
    "        feature: string\n",
    "        \n",
    "        split_point: int/float/string (can only be discrete)\n",
    "        \n",
    "        \"\"\"\n",
    "        m,n = X.shape\n",
    "        \n",
    "        X_cond = X[:,feature]\n",
    "\n",
    "        for name in range(m):\n",
    "            if X_cond[name] != split_point:\n",
    "                X_cond[name] = 1\n",
    "            else:\n",
    "                X_cond[name] = 0\n",
    "        # Create a nested dictionary, \n",
    "        # {\"X_cate\":{\"label\":count}}\n",
    "        \n",
    "        # Find unique category in X_cond\n",
    "        X_cate = set(X_cond)\n",
    "\n",
    "        \n",
    "        # Find unique label in y_label\n",
    "        label = set(y_label)\n",
    "        \n",
    "        \n",
    "        \n",
    "        labelCounts = collections.defaultdict(dict)\n",
    "        \n",
    "        for cate in X_cate:\n",
    "            for lab in label:\n",
    "                labelCounts[cate][lab] = 0\n",
    "                    \n",
    "        # Number of samples in a category        \n",
    "        n_sample_cate = collections.defaultdict(int)\n",
    "        \n",
    "        # stores the counts of each label corresponding to each category of a feature\n",
    "        for ind in range(m):\n",
    "            labelCounts[X_cond[ind]][y_label[ind]] += 1\n",
    "            n_sample_cate[X_cond[ind]] += 1\n",
    "            \n",
    "        # calculate the new entropy\n",
    "        condShannonEntropy = 0.0\n",
    "        # calculate the denomonator\n",
    "        H = 0.0\n",
    "        for cate_key,label_dict in labelCounts.items():\n",
    "\n",
    "        \n",
    "            for label_key in label_dict:\n",
    "                # calculate occurrence\n",
    "                prob = float(labelCounts[cate_key][label_key]/n_sample_cate[cate_key])\n",
    "\n",
    "                if cate_key not in n_sample_cate:\n",
    "                    n_sample_cate[cate_key] = 0\n",
    "                \n",
    "                if prob == 0:\n",
    "                    continue\n",
    "                # calculate entropy\n",
    "                condShannonEntropy -= prob * np.log2(prob)\n",
    "\n",
    "            y_prob = float(n_sample_cate[cate_key]/m)\n",
    "            if y_prob == 0:\n",
    "                continue\n",
    "            # calculate denomonator H\n",
    "            H -=  y_prob * np.log2(y_prob)    \n",
    "        \n",
    "        return condShannonEntropy, H\n",
    "\n",
    "    # Select Feature from Max Information Gain\n",
    "    \n",
    "    def _feature_Select_Info_Gain(self, X, y_label):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------------\n",
    "        X: {array-like}, shape = [n_samples, n_features]\n",
    "        \n",
    "        y_label: {array-like}, shape = [n_samples, 1]\n",
    "        \n",
    "        \"\"\"\n",
    "        m,n = X.shape\n",
    "        #informationGain = []\n",
    "        shannonEntropy = self.calcShannonEntropy(y_label)\n",
    "        \n",
    "        max_gain = 0.0\n",
    "        best_feature = 0\n",
    "        best_split = None\n",
    "        \n",
    "        for feature in range(n):\n",
    "            \n",
    "            X_cate = set(X[:,feature])\n",
    "            \n",
    "            for split_point in list(X_cate)[1:]:   \n",
    "                \n",
    "                # condShannonEntropy, H \n",
    "                \n",
    "                condShannonEntrop, H = self.conditionalEnropy(X,y_label,feature,split_point)\n",
    "                \n",
    "                informationGain = shannonEntropy - condShannonEntrop\n",
    "\n",
    "                \n",
    "                if informationGain > max_gain:\n",
    "                    max_gain = informationGain \n",
    "                    best_split = split_point\n",
    "                    best_feature = feature\n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "            print('\\n')\n",
    "            print(\"The best feature is {}, and it's best split point is {}\".format(best_feature,best_split))\n",
    "            print('\\n')\n",
    "        \n",
    "        return best_feature, best_split\n",
    "    \n",
    "    # Select Feature from Max Information Gain Ratio\n",
    "    \n",
    "    def _feature_Select_Info_Gain_Ratio(self, X, y_label):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        m,n = X.shape\n",
    "        GainRatio = []\n",
    "        shannonEntropy = self.calcShannonEntropy(y_label)\n",
    "        \n",
    "        max_gain = 0.0\n",
    "        best_feature = 0\n",
    "        best_split = None\n",
    "        \n",
    "        for feature in range(n):\n",
    "            X_cate = set(X[:,feature])\n",
    "            \n",
    "            for split_point in list(X_cate)[1:]: \n",
    "            \n",
    "                condShannonEntropy, H = self.conditionalEnropy(X,y_label,feature,split_point)\n",
    "#                 GainRatio.append((shannonEntropy - condShannonEntropy)/H)\n",
    "                gainRatio = (shannonEntropy - condShannonEntropy)/H\n",
    "\n",
    "\n",
    "                if gainRatio > max_gain:\n",
    "                    max_gain = gainRatio \n",
    "                    best_split = split_point\n",
    "                    best_feature = feature\n",
    "\n",
    "#         feature = GainRatio.index(max(GainRatio))\n",
    "        if self.verbose == 1:\n",
    "            print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "            print('\\n')\n",
    "            print(\"The best feature is {}, and it's best split point is {}\".format(best_feature,best_split))\n",
    "            print('\\n')\n",
    "        return best_feature,best_split\n",
    "    \n",
    "    \n",
    "    # Select Feature from Min Gini\n",
    "    def gini_Impurity(self, y_labels):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------------\n",
    "        y_labels: {array-like}, shape = [n_samples, 1]\n",
    "            Dateset matrix, where 'n_samples' is the number of samples \n",
    "            and one column of labels\n",
    "        \n",
    "        Return\n",
    "        ------------------\n",
    "        giniImpurity: float\n",
    "        \n",
    "        \"\"\"\n",
    "        m = len(y_labels)\n",
    "        \n",
    "        labelCounts = collections.defaultdict(int)\n",
    "        \n",
    "        # the number of unique elements and their occurrence\n",
    "        for label in y_labels:\n",
    "            if label not in labelCounts:\n",
    "                labelCounts[label] = 0\n",
    "            labelCounts[label] += 1\n",
    "            \n",
    "        # calculate gini index\n",
    "        gini = 1.0\n",
    "        for key in labelCounts:\n",
    "            # calculate occurrence\n",
    "            prob = float(labelCounts[key]/m)\n",
    "            # calculate entropy\n",
    "            gini -= prob ** 2 \n",
    "        \n",
    "        return gini\n",
    "    \n",
    "    \n",
    "    def total_Gini_Impurity(self, X, y_label, feature):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------------\n",
    "        X: {array-like} discrete features\n",
    "        preprocessed dataset, could not deal with continuous features, \n",
    "        and categorical feature should be better as binary form\n",
    "        \n",
    "        y_label: {array-like}\n",
    "        \n",
    "        feature: string\n",
    "        \n",
    "        Return\n",
    "        ------------------\n",
    "        split_point: it could be several different types: float, int, tring\n",
    "        \n",
    "        \"\"\"\n",
    "        m,n = X.shape\n",
    "        \n",
    "        X_cond = X[:,feature]\n",
    "        \n",
    "        # Create a nested dictionary, \n",
    "        # {\"X_cate\":{\"label\":count}}\n",
    "        \n",
    "        # Find unique category in X_cond\n",
    "        X_cate = sorted(set(X_cond))\n",
    "        # Find unique label in y_label\n",
    "        label = set(y_label)\n",
    "        \n",
    "        split_point = collections.defaultdict(float)\n",
    "        \n",
    "        for i in list(X_cate)[1:]:\n",
    "            \"\"\"\n",
    "            X_cond[i] == split point\n",
    "            if yes, then left child, if no, then right child.\n",
    "            \"\"\"\n",
    "\n",
    "            left_child = y_label[np.where(X_cond <= i)[0]]\n",
    "            left_gini = self.gini_Impurity(left_child)\n",
    "\n",
    "            right_child = y_label[np.where(X_cond > i)[0]]\n",
    "            right_gini = self.gini_Impurity(right_child)\n",
    "            \n",
    "            totalGiniScore = float(len(left_child) * left_gini) + float(len(right_child)*right_gini)\n",
    "\n",
    "            split_point[i] = totalGiniScore\n",
    "            \n",
    "        split =  min(split_point, key= split_point.get)\n",
    "     \n",
    "        return split_point[split], split\n",
    "        \n",
    "        \n",
    "    def _feature_Select_Gini(self,X,y_labels):\n",
    "        m,n = X.shape\n",
    "        GiniScore = []\n",
    "        Splits = []\n",
    "        \n",
    "        for feature in range(n):\n",
    "            gini, split_point = self.total_Gini_Impurity(X,y_labels,feature)\n",
    "            GiniScore.append(gini)\n",
    "            Splits.append(split_point)\n",
    "        \n",
    "        best_feature = GiniScore.index(min(GiniScore))\n",
    "        best_split = Splits[feature]\n",
    "        if self.verbose == 1:\n",
    "            print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "            print('\\n')\n",
    "            print(\"The best feature is {}, and it's best split point is {}\".format(best_feature,best_split))\n",
    "            print('\\n')   \n",
    "        return best_feature, best_split\n",
    "        \n",
    "    def featureSelect(self, X, y_label):\n",
    "        \n",
    "        best_feature, best_split = self.feature_split(X, y_label)\n",
    "        \n",
    "        return best_feature, best_split\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Step2:\n",
    "    BUILD A TREE\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def buildTree(x_data,y_labels):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSet():\n",
    "    dataSet = np.array([[2, 2],\n",
    "            [2, 1],\n",
    "            [0, 2],\n",
    "            [0, 1],\n",
    "            [0, 1]])\n",
    "    \n",
    "    labels = np.array(['yes',\n",
    "              'yes',\n",
    "              'no',\n",
    "              'no',\n",
    "              'no'])\n",
    "    return dataSet, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "\n",
      "The best feature is 0, and it's best split point is 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet ,labels =  createDataSet()\n",
    "dt = DecisionTreeClassifier(criterion='Gini')\n",
    "dt.featureSelect(dataSet,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
